# 曲线拟合的一些算法

## 最小二乘法

对于数据样本不多的情况下，最小二乘拟合曲线是最优雅的解法。

```PYTHON
    # 调用scipy库函数
    p0 = [0.1, -0.01] # 拟合的初始参数设置
    para = leastsq(error, p0, args=(X, Y))  # 进行拟合
    print(para)
    Y2 = Fun(para[0],X) 
    ax.plot(X, Y2, c='g') 
    
    # 矩阵解法
    X_b = np.ones([num_,2]) # 第二个维度是bias
    X_b[:,0] = X**3 # 此处体现的就是目标函数是 y=a*x^3+b
    w_ = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(Y) # 直接求解出参数
```

## 梯度下降法

- [用梯度下降的方式来拟合曲线](https://blog.csdn.net/qq_40622955/article/details/129911670)

```python
# 梯度下降算法来解决最优化问题，求损失函数的最小值
def gradient(X, Y, lr, m, iter_numbers):
    para = np.ones([2,1])
    # para = np.array([1.5, 26.2]).reshape([2,1])
    Y = Y.reshape([m, 1])
       
    X = X**3 # 目标函数y = a*x^3 +b
    # X2, scale = minmax_scaling(X)  # Min-Max Normalization
    # X2, scale = normalization_(X)  # Z-Score Normalization
    X2, scale= X, 1
    
    X_b = np.ones([num_,2]) 
    X_b[:,0] = X2
    X_trans=X_b.transpose()  # 转化为列向量
    last_loss = 1e5
    count = 1
    while 1:          
        Y_hat = np.dot(X_b, para)  # 正向传播
        loss = ((Y - Y_hat) ** 2).mean()
        print("iteration:%d / Cost:%f"%(count, loss)) 
        gradient = -3 * X_b.T.dot(Y - Y_hat) / m  # 
        para = para - gradient * lr
        
        if abs(last_loss - loss) < 1e-8:
            break
        
        last_loss = loss
        count = count + 1
        if count > 200e4:
            break
        if np.isnan(loss):
            break
        
    print(para, scale)   
    para[0] = para[0] / scale
    return para
```

在使用梯度下降法进行曲线参数回归的时候，会遇到梯度爆炸的情况。

主要是由于特征的范围过大，比如这里的x输入范围从1-100的话，其x^3的范围为[1， 1000000]。一定情况下，可以通过减小学习率使得算法正常运行，但更常见的做法应当是对**数据进行归一化处理**。


## 数据归一化

- [数据预处理：归一化和标准化](https://zhuanlan.zhihu.com/p/296252799)

## 梯度下降法资料

- [谈谈优化算法之一（动量法、Nesterov法、自然梯度法）](https://zhuanlan.zhihu.com/p/60088231)
  
- [简单认识Adam优化器](https://zhuanlan.zhihu.com/p/32698042)