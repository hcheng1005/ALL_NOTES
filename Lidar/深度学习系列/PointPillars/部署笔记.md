# PointPillars部署笔记


- [参考工程](#参考工程)
- [ONNX模型导出和TRT文件生成](#onnx模型导出和trt文件生成)
- [C++代码工程解读](#c代码工程解读)
  - [TRT模型加载](#trt模型加载)
    - [GPT解释](#gpt解释)
  - [GPT：加载TensorRT模型的标准流程](#gpt加载tensorrt模型的标准流程)
- [模型推理](#模型推理)
  - [GPT解释](#gpt解释-1)
- [相关函数解释](#相关函数解释)
  - [cudaDeviceSynchronize()](#cudadevicesynchronize)

---

## 参考工程

Code: [CUDA-PointPillars](https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars/tree/main)

Code: [OpenPCDet](https://github.com/hcheng1005/OpenPCDet)

Code：[PointPillars_MultiHead_40FPS](https://github.com/hcheng1005/PointPillars_MultiHead_40FPS)

## ONNX模型导出和TRT文件生成
详见[PointPillars_MultiHead_40FPS](https://github.com/hcheng1005/PointPillars_MultiHead_40FPS)

## C++代码工程解读

### TRT模型加载
在`void PointPillars::InitTRT(const bool use_onnx)`函数中，有两种模型加载方式：`OnnxToTRTModel`和`EngineToTRTModel`。一般情况下，采用`EngineToTRTModel`方式。

代码：
```c++
void PointPillars::EngineToTRTModel(
    const std::string &engine_file ,     
    nvinfer1::ICudaEngine** engine_ptr)  {
    int verbosity = static_cast<int>(nvinfer1::ILogger::Severity::kWARNING);
    std::stringstream gieModelStream; 
    gieModelStream.seekg(0, gieModelStream.beg); 

    std::ifstream cache(engine_file); 
    gieModelStream << cache.rdbuf();
    cache.close(); 
    nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(g_logger_); 

    if (runtime == nullptr) {
        std::string msg("failed to build runtime parser");
        g_logger_.log(nvinfer1::ILogger::Severity::kERROR, msg.c_str());
        exit(EXIT_FAILURE);
    }
    gieModelStream.seekg(0, std::ios::end);
    const int modelSize = gieModelStream.tellg(); 

    gieModelStream.seekg(0, std::ios::beg);
    void* modelMem = malloc(modelSize); 
    gieModelStream.read((char*)modelMem, modelSize);    
    nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(modelMem, modelSize, NULL); 
    if (engine == nullptr) {
        std::string msg("failed to build engine parser");
        g_logger_.log(nvinfer1::ILogger::Severity::kERROR, msg.c_str());
        exit(EXIT_FAILURE);
    }
    *engine_ptr = engine;
}
```
#### GPT解释

这段代码是一个函数，名为`PointPillars::EngineToTRTModel`。它的作用是将一个TensorRT引擎文件加载到内存中，并返回一个指向该引擎的指针。

函数的参数包括一个`engine_file`，表示TensorRT引擎文件的路径，以及一个指向`nvinfer1::ICudaEngine`指针的指针`engine_ptr`，用于存储加载后的引擎。

函数的实现过程如下：

1. 将日志级别设置为警告级别。
2. 创建一个字符串流对象`gieModelStream`，用于存储引擎文件的数据流。
3. 打开引擎文件，将文件内容读取到`gieModelStream`中。
4. 关闭文件。
5. 创建一个TensorRT运行时对象`runtime`。
6. 如果`runtime`为空指针，则输出错误信息并退出程序。
7. 定位到`gieModelStream`的末尾，获取引擎文件的大小。
8. 定位到`gieModelStream`的开头。
9. 分配足够大小的内存块`modelMem`，用于存储引擎文件的数据。
10. 从`gieModelStream`中读取引擎文件的数据到`modelMem`中。
11. 使用`runtime`对象的`deserializeCudaEngine`方法，将`modelMem`中的数据反序列化为一个TensorRT引擎对象`engine`。
12. 如果`engine`为空指针，则输出错误信息并退出程序。
13. 将`engine`赋值给`engine_ptr`指向的指针。
14. 函数执行结束。

总的来说，这段代码的功能是将TensorRT引擎文件加载到内存中，并返回一个指向该引擎的指针。

### GPT：加载TensorRT模型的标准流程

当加载TensorRT模型时，通常会遵循以下标准流程：

1. 创建`nvinfer1::IRuntime`对象：使用`nvinfer1::createInferRuntime`函数创建一个TensorRT的运行时对象。

2. 创建`nvinfer1::ICudaEngine`对象：使用`runtime->deserializeCudaEngine`方法将TensorRT引擎文件反序列化为一个CUDA引擎对象。

3. 创建`nvinfer1::IExecutionContext`对象（可选）：如果需要执行推理操作，可以使用`engine->createExecutionContext`方法创建一个执行上下文对象。

4. 分配内存并传输数据：根据模型的输入要求，分配足够的内存，并将输入数据传输到GPU内存中。

5. 执行推理：使用`context->execute`方法执行推理操作，将输入数据传递给模型，获取输出结果。

6. 处理输出结果：根据模型的输出要求，从GPU内存中获取输出数据，并进行后续处理或分析。

7. 释放资源：释放分配的内存，并销毁创建的对象，包括`nvinfer1::IExecutionContext`、`nvinfer1::ICudaEngine`和`nvinfer1::IRuntime`。

需要注意的是，加载TensorRT模型的具体流程可能会因应用场景和需求的不同而有所变化。例如，如果模型中使用了自定义插件，还需要注册和设置插件工厂。此外，还可以对TensorRT的配置进行调整，以优化推理性能和精度。

以上是一个基本的加载TensorRT模型的标准流程，具体实现时需要根据应用的需求进行适当的调整和扩展。


在C++工程代码中，当执行完`PointPillars::EngineToTRTModel`（加载完毕trt模型后），有如下代码：
```c++
pfe_context_ = pfe_engine_->createExecutionContext();
```

之前，完成TRT模型的完整加载，后续可使用`pfe_context_`进行推理。

## 模型推理
```c++
// [STEP 3] : pfe forward
cudaStream_t stream;
GPU_CHECK(cudaStreamCreate(&stream));
auto pfe_start = std::chrono::high_resolution_clock::now();
GPU_CHECK(cudaMemcpyAsync(pfe_buffers_[0], dev_pfe_gather_feature_,
                        kMaxNumPillars * kMaxNumPointsPerPillar * kNumGatherPointFeature * sizeof(float), ///kNumGatherPointFeature
                        cudaMemcpyDeviceToDevice, stream));
pfe_context_->enqueueV2(pfe_buffers_, stream, nullptr);
cudaDeviceSynchronize();
auto pfe_end = std::chrono::high_resolution_clock::now();
```

### GPT解释
这段代码是一个CUDA程序，用于在GPU上执行一些计算任务。让我逐步解释一下：

1. `cudaStream_t stream;`：定义了一个CUDA流对象，用于管理GPU上的并行操作。

2. `GPU_CHECK(cudaStreamCreate(&stream));`：创建了一个CUDA流，并将其存储在`stream`变量中。这个函数会检查CUDA操作是否成功，并在失败时抛出错误。

3. `auto pfe_start = std::chrono::high_resolution_clock::now();`：记录了当前时间点，用于计算代码执行的时间。

4. `GPU_CHECK(cudaMemcpyAsync(pfe_buffers_[0], dev_pfe_gather_feature_, kMaxNumPillars * kMaxNumPointsPerPillar * kNumGatherPointFeature * sizeof(float), cudaMemcpyDeviceToDevice, stream));`：在GPU上异步地将`dev_pfe_gather_feature_`的数据复制到`pfe_buffers_[0]`中。这个函数会检查CUDA操作是否成功，并在失败时抛出错误。

5. `pfe_context_->enqueueV2(pfe_buffers_, stream, nullptr);`：将`pfe_buffers_`中的数据提交给一个名为`pfe_context_`的计算上下文进行处理。这个函数会在GPU上执行计算任务。

6. `cudaDeviceSynchronize();`：等待GPU上的所有操作完成，保证前面的计算任务已经执行完毕。

7. `auto pfe_end = std::chrono::high_resolution_clock::now();`：记录了当前时间点，用于计算代码执行的时间。

这段代码的主要目的是在GPU上执行一些计算任务，并记录执行时间。

## 相关函数解释

### cudaDeviceSynchronize()

`cudaDeviceSynchronize()`函数的作用是等待当前设备上的所有CUDA任务完成。它会阻塞CPU线程，直到GPU上的所有任务都执行完毕。

在CUDA程序中，通常会使用异步操作来提高并行性和性能。这意味着在CPU线程继续执行之前，GPU上的某些操作可能还没有完成。为了确保CPU和GPU之间的同步，可以使用`cudaDeviceSynchronize()`函数来等待GPU上的所有操作完成。

当调用`cudaDeviceSynchronize()`时，CPU线程会暂停执行，直到GPU上的所有任务都完成。这样可以确保在继续执行后续代码之前，GPU上的计算结果已经准备好并可用。

在上述代码中，`cudaDeviceSynchronize()`的目的是确保在记录代码执行时间之前，GPU上的所有计算任务都已经完成。这样可以准确地测量代码的执行时间。